{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     5,
     19,
     26,
     37,
     62,
     72,
     84
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.signal import lfilter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def update_target_graph(from_scope, to_scope):\n",
    "    \"\"\"\n",
    "    Copies one set of variables to another.\n",
    "    Used to set worker network parameters to those of global network.\n",
    "    \"\"\"\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var, to_var in zip(from_vars, to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder\n",
    "\n",
    "\n",
    "def discount(x, gamma):\n",
    "    \"\"\"\n",
    "    Discounting function used to calculate discounted returns.\n",
    "    \"\"\"\n",
    "    return lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    \"\"\"\n",
    "    Used to initialize weights for policy and value output layers\n",
    "    \"\"\"\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer\n",
    "\n",
    "\n",
    "def look_for_folder(main_folder='priors/', exp=''):\n",
    "    \"\"\"\n",
    "    looks for a given folder and returns it.\n",
    "    If it cannot find it, returns possible candidates\n",
    "    \"\"\"\n",
    "    data_path = ''\n",
    "    possibilities = []\n",
    "    for root, dirs, files in os.walk(main_folder):\n",
    "        ind = root.rfind('/')\n",
    "        possibilities.append(root[ind+1:])\n",
    "        if root[ind+1:] == exp:\n",
    "            data_path = root\n",
    "            break\n",
    "\n",
    "    if data_path == '':\n",
    "        candidates = difflib.get_close_matches(exp, possibilities,\n",
    "                                               n=1, cutoff=0.)\n",
    "        print(exp + ' NOT FOUND IN ' + main_folder)\n",
    "        if len(candidates) > 0:\n",
    "            print('possible candidates:')\n",
    "            print(candidates)\n",
    "\n",
    "    return data_path\n",
    "\n",
    "\n",
    "def list_str(l):\n",
    "    \"\"\"\n",
    "    list to str\n",
    "    \"\"\"\n",
    "    nice_string = str(l[0])\n",
    "    for ind_el in range(1, len(l)):\n",
    "        nice_string += '_'+str(l[ind_el])\n",
    "    return nice_string\n",
    "\n",
    "\n",
    "def num2str(num):\n",
    "    \"\"\"\n",
    "    pass big number to thousands\n",
    "    \"\"\"\n",
    "    return str(int(num/1000))+'K'\n",
    "\n",
    "\n",
    "def rm_lines():\n",
    "    ax = plt.gca()\n",
    "    ax.clear()\n",
    "\n",
    "\n",
    "def plot_trials_start(trials, minimo, maximo, num_steps, color='k'):\n",
    "    trials = np.nonzero(trials)[0] - 0.5\n",
    "    cond = np.logical_and(trials >= 0, trials <= num_steps)\n",
    "    trials = trials[np.where(cond)]\n",
    "    for ind_tr in range(len(trials)):\n",
    "        plt.plot([trials[ind_tr], trials[ind_tr]], [minimo, maximo],\n",
    "                 '--'+color, lw=1)\n",
    "    plt.xlim(0-0.5, num_steps-0.5)\n",
    "\n",
    "\n",
    "def folder_name(gamma=0.8, up_net=5, trial_dur=10,\n",
    "                rep_prob=(.2, .8), exp_dur=10**6, rewards=(-0.1, 0.0, 1.0, -1.0),\n",
    "                block_dur=200, num_units=32,\n",
    "                stim_ev=0.5, network='ugru', learning_rate=10e-3,\n",
    "                instance=0, main_folder='', t_flag=''):\n",
    "    return main_folder + '/td_' + str(trial_dur) + '_rp_' +\\\n",
    "        str(list_str(rep_prob)) + '_r_' +\\\n",
    "        str(list_str(rewards)) + '_bd_' + str(block_dur) +\\\n",
    "        '_ev_' + str(stim_ev) + '_g_' + str(gamma) + '_lr_' + str(learning_rate) +\\\n",
    "        '_nu_' + str(num_units) + '_un_' + str(up_net) +\\\n",
    "        '_net_' + str(network) + '_ed_' + str(exp_dur) + '_' + str(instance) + t_flag + '/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     41,
     51,
     95,
     204,
     243
    ],
    "hidden": true,
    "init_cell": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.special import erf\n",
    "\n",
    "\n",
    "\n",
    "def plot_learning(performance, evidence, stim_position, action):\n",
    "    \"\"\"\n",
    "    plots RNN and ideal observer performances.\n",
    "    The function assumes that a figure has been created\n",
    "    before it is called.\n",
    "    \"\"\"\n",
    "    # remove all previous plots\n",
    "    rm_lines()\n",
    "    # ideal observer choice\n",
    "    io_choice = evidence < 0\n",
    "    io_performance = io_choice == stim_position\n",
    "    # save the mean performances\n",
    "    RNN_perf = np.mean(performance[:, 2000:].flatten())\n",
    "    io_perf = np.mean(io_performance[:, 2000:].flatten())\n",
    "\n",
    "    w_conv = 200  # this is for the smoothing\n",
    "    # plot smoothed performance\n",
    "    performance_smoothed = np.convolve(np.mean(performance, axis=0),\n",
    "                                       np.ones((w_conv,))/w_conv,\n",
    "                                       mode='valid')\n",
    "    plt.plot(performance_smoothed, color=(0.39, 0.39, 0.39), lw=0.5,\n",
    "             label='RNN perf. (' + str(round(RNN_perf, 3)) + ')')\n",
    "\n",
    "    # plot ideal observer performance\n",
    "    io_perf_smoothed = np.convolve(np.mean(io_performance, axis=0),\n",
    "                                   np.ones((w_conv,))/w_conv,\n",
    "                                   mode='valid')\n",
    "    plt.plot(io_perf_smoothed, color=(1, 0.8, 0.5), lw=0.5,\n",
    "             label='Ideal Obs. perf. (' + str(round(io_perf, 3)) + ')')\n",
    "    # plot 0.25, 0.5 and 0.75 performance lines\n",
    "    plot_fractions([0, performance.shape[1]])\n",
    "    plt.title('performance')\n",
    "    plt.xlabel('trials')\n",
    "    plt.legend()\n",
    "\n",
    "def plot_fractions(lims):\n",
    "    \"\"\"\n",
    "    plot dashed lines for 0.25, 0.5 and 0.75\n",
    "    \"\"\"\n",
    "    plt.plot(lims, [0.25, 0.25], '--k', lw=0.25)\n",
    "    plt.plot(lims, [0.5, 0.5], '--k', lw=0.25)\n",
    "    plt.plot(lims, [0.75, 0.75], '--k', lw=0.25)\n",
    "    plt.xlim(lims[0], lims[1])\n",
    "\n",
    "\n",
    "def plot_psychometric_curves(evidence, performance, action,\n",
    "                             blk_dur=200,\n",
    "                             plt_av=True, figs=True):\n",
    "    \"\"\"\n",
    "    plots psychometric curves\n",
    "    - evidence for right VS prob. of choosing right\n",
    "    - evidence for repeating side VS prob. of repeating\n",
    "    - same as above but conditionated on hits and fails\n",
    "    The function assumes that a figure has been created \n",
    "    before it is called.\n",
    "    \"\"\"\n",
    "    # build the mat that indicates the current block\n",
    "    rep_prob = build_block_mat(evidence.shape, blk_dur)\n",
    "\n",
    "    # repeating probs. values\n",
    "    probs_vals = np.unique(rep_prob)\n",
    "    assert len(probs_vals) <= 2\n",
    "    colors = [[1, 0, 0], [0, 0, 1]]\n",
    "    if figs:\n",
    "        rows = 2\n",
    "        cols = 2\n",
    "    else:\n",
    "        rows = 0\n",
    "        cols = 0\n",
    "\n",
    "    data = {}\n",
    "    for ind_sp in range(4):\n",
    "        plt.subplot(rows, cols, ind_sp+1)\n",
    "        # remove all previous plots\n",
    "        rm_lines()\n",
    "    for ind_blk in range(len(probs_vals)):\n",
    "        # filter data\n",
    "        inds = (rep_prob == probs_vals[ind_blk])\n",
    "        evidence_block = evidence[inds]\n",
    "        performance_block = performance[inds]\n",
    "        action_block = action[inds]\n",
    "        data = get_psyCho_curves_data(performance_block,\n",
    "                                      evidence_block, action_block,\n",
    "                                      probs_vals[ind_blk],\n",
    "                                      rows, cols, figs, colors[ind_blk],\n",
    "                                      plt_av, data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_psyCho_curves_data(performance, evidence, action, prob,\n",
    "                           rows, cols, figs, color, plt_av, data):\n",
    "    \"\"\"\n",
    "    plot psychometric curves for:\n",
    "    right evidence VS prob. choosing right\n",
    "    repeating evidence VS prob. repeating\n",
    "    repeating evidence VS prob. repeating (conditionated on previous correct)\n",
    "    repeating evidence VS prob. repeating (conditionated on previous wrong)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. RIGHT EVIDENCE VS PROB. CHOOSING RIGHT\n",
    "    # get the action\n",
    "    right_choice = action == 0\n",
    "\n",
    "    # associate invalid trials (network fixates) with random choice\n",
    "    right_choice[action == 2] = evidence[action == 2] > 0\n",
    "    # np.random.choice([0, 1], size=(np.sum(action.flatten() == 2),))\n",
    "\n",
    "    # convert the choice to float and flatten it\n",
    "    right_choice = [float(x) for x in right_choice]\n",
    "    right_choice = np.asarray(right_choice)\n",
    "    # fit and plot\n",
    "    if figs:\n",
    "        plt.subplot(rows, cols, 1)\n",
    "        plt.xlabel('right evidence')\n",
    "        plt.ylabel('prob. right')\n",
    "    popt, pcov, av_data =\\\n",
    "        fit_and_plot(evidence, right_choice,\n",
    "                     plt_av, color=color, figs=figs)\n",
    "\n",
    "    data['popt_rightProb_' + str(prob)] = popt\n",
    "    data['pcov_rightProb_' + str(prob)] = pcov\n",
    "    data['av_rightProb_' + str(prob)] = av_data\n",
    "\n",
    "    # 2. REPEATING EVIDENCE VS PROB. REPEATING\n",
    "    # I add a random choice to the beginning of the choice matrix\n",
    "    # and differentiate to see when the network is repeating sides\n",
    "    repeat = np.concatenate(\n",
    "        (np.array(np.random.choice([0, 1])).reshape(1,),\n",
    "         right_choice))\n",
    "    repeat = np.diff(repeat) == 0\n",
    "    # right_choice_repeating is just the original right_choice mat\n",
    "    # but shifted one element to the left.\n",
    "    right_choice_repeating = np.concatenate(\n",
    "        (np.array(np.random.choice([0, 1])).reshape(1, ),\n",
    "         right_choice[:-1]))\n",
    "    # the rep. evidence is the original evidence with a negative sign\n",
    "    # if the repeating side is the left one\n",
    "    rep_ev_block = evidence *\\\n",
    "        (-1)**(right_choice_repeating == 0)\n",
    "    # fitting\n",
    "    if figs:\n",
    "        label_aux = 'p. rep.: ' + str(prob)\n",
    "        plt.subplot(rows, cols, 2)\n",
    "        #         plt.xlabel('repetition evidence')\n",
    "        #         plt.ylabel('prob. repetition')\n",
    "    else:\n",
    "        label_aux = ''\n",
    "    popt, pcov, av_data =\\\n",
    "        fit_and_plot(rep_ev_block, repeat,\n",
    "                     plt_av, color=color,\n",
    "                     label=label_aux, figs=figs)\n",
    "\n",
    "    data['popt_repProb_'+str(prob)] = popt\n",
    "    data['pcov_repProb_'+str(prob)] = pcov\n",
    "    data['av_repProb_'+str(prob)] = av_data\n",
    "\n",
    "    # plot psycho-curves conditionated on previous performance\n",
    "    # get previous trial performance\n",
    "    prev_perf = np.concatenate(\n",
    "        (np.array(np.random.choice([0, 1])).reshape(1,),\n",
    "         performance[:-1]))\n",
    "    # 3. REPEATING EVIDENCE VS PROB. REPEATING (conditionated on previous correct)\n",
    "    # fitting\n",
    "    mask = prev_perf == 1\n",
    "    if figs:\n",
    "        plt.subplot(rows, cols, 3)\n",
    "        plt.xlabel('repetition evidence')\n",
    "        plt.ylabel('prob. repetition')\n",
    "        #         plt.title('Prev. hit')\n",
    "    popt, pcov, av_data =\\\n",
    "        fit_and_plot(rep_ev_block[mask], repeat[mask],\n",
    "                     plt_av, color=color,\n",
    "                     label=label_aux, figs=figs)\n",
    "\n",
    "    data['popt_repProb_hits_'+str(prob)] = popt\n",
    "    data['pcov_repProb_hits_'+str(prob)] = pcov\n",
    "    data['av_repProb_hits_'+str(prob)] = av_data\n",
    "\n",
    "    # 4. REPEATING EVIDENCE VS PROB. REPEATING (conditionated on previous wrong)\n",
    "    # fitting\n",
    "    mask = prev_perf == 0\n",
    "    if figs:\n",
    "        plt.subplot(rows, cols, 4)\n",
    "        plt.xlabel('repetition evidence')\n",
    "        #         plt.ylabel('prob. repetition')\n",
    "        #         plt.title('Prev. fail')\n",
    "    popt, pcov, av_data =\\\n",
    "        fit_and_plot(rep_ev_block[mask], repeat[mask],\n",
    "                     plt_av, color=color,\n",
    "                     label=label_aux, figs=figs)\n",
    "\n",
    "    data['popt_repProb_fails_'+str(prob)] = popt\n",
    "    data['pcov_repProb_fails_'+str(prob)] = pcov\n",
    "    data['av_repProb_fails_'+str(prob)] = av_data\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def fit_and_plot(evidence, choice, plt_av=False,\n",
    "                 color=(0, 0, 0), label='', figs=False):\n",
    "    \"\"\"\n",
    "    uses curve_fit to fit the evidence/choice provided to a probit function\n",
    "    that takes into account the lapse rates\n",
    "    it also plots the corresponding fit and, if plt_av=True, plots the\n",
    "    average choice values for different windows of the evidence\n",
    "    \"\"\"\n",
    "    if evidence.shape[0] > 10 and len(np.unique(choice)) == 2:\n",
    "        # fit\n",
    "        popt, pcov = curve_fit(probit_lapse_rates,\n",
    "                               evidence, choice, maxfev=10000)\n",
    "    # plot averages\n",
    "        if plt_av:\n",
    "            av_data = plot_psychoCurves_averages(evidence, choice,\n",
    "                                                 color=color, figs=figs)\n",
    "        else:\n",
    "            av_data = {}\n",
    "        # plot obtained probit function\n",
    "        if figs:\n",
    "            x = np.linspace(np.min(evidence),\n",
    "                            np.max(evidence), 50)\n",
    "            # get the y values for the fitting\n",
    "            y = probit_lapse_rates(x, popt[0], popt[1], popt[2], popt[3])\n",
    "            if label == '':\n",
    "                plt.plot(x, y, color=color, lw=0.5)\n",
    "            else:\n",
    "                plt.plot(x, y, color=color,  label=label\n",
    "                         + ' b: ' + str(round(popt[1], 3)), lw=0.5)\n",
    "                # plt.legend(loc=\"lower right\")\n",
    "            plot_dashed_lines(-np.max(evidence), np.max(evidence))\n",
    "    else:\n",
    "        av_data = {}\n",
    "        popt = [0, 0, 0, 0]\n",
    "        pcov = 0\n",
    "        print('not enough data!')\n",
    "    return popt, pcov, av_data\n",
    "\n",
    "\n",
    "def plot_psychoCurves_averages(x_values, y_values,\n",
    "                               color=(0, 0, 0), figs=False):\n",
    "    \"\"\"\n",
    "    plots average values of y_values for 10 (num_values) different windows\n",
    "    in x_values\n",
    "    \"\"\"\n",
    "    num_values = 10\n",
    "    conf = 0.95\n",
    "    x, step = np.linspace(np.min(x_values), np.max(x_values),\n",
    "                          num_values, retstep=True)\n",
    "    curve_mean = []\n",
    "    curve_std = []\n",
    "    # compute mean for each window\n",
    "    for ind_x in range(num_values-1):\n",
    "        inds = (x_values >= x[ind_x])*(x_values < x[ind_x+1])\n",
    "        mean = np.mean(y_values[inds])\n",
    "        curve_mean.append(mean)\n",
    "        curve_std.append(conf*np.sqrt(mean*(1-mean)/np.sum(inds)))\n",
    "\n",
    "    if figs:\n",
    "        # make color weaker\n",
    "        # np.max(np.concatenate((color, [1, 1, 1]), axis=0), axis=0)\n",
    "        color_w = np.array(color) + 0.5\n",
    "        color_w[color_w > 1] = 1\n",
    "        # plot\n",
    "        plt.errorbar(x[:-1] + step / 2, curve_mean, curve_std,\n",
    "                     color=color_w, marker='+', linestyle='')\n",
    "\n",
    "    # put values in a dictionary\n",
    "    av_data = {'mean': curve_mean, 'std': curve_std, 'x': x[:-1]+step/2}\n",
    "    return av_data\n",
    "\n",
    "\n",
    "def build_block_mat(shape, block_dur):\n",
    "    # build rep. prob vector\n",
    "    rp_mat = np.zeros(shape)\n",
    "    a = np.arange(shape[1])\n",
    "    b = np.floor(a/block_dur)\n",
    "    rp_mat[:, b % 2 == 0] = 1\n",
    "    return rp_mat\n",
    "\n",
    "\n",
    "def probit_lapse_rates(x, beta, alpha, piL, piR):\n",
    "    piR = 0\n",
    "    piL = 0\n",
    "    probit_lr = piR + (1 - piL - piR) * probit(x, beta, alpha)\n",
    "    return probit_lr\n",
    "\n",
    "\n",
    "def probit(x, beta, alpha):\n",
    "    probit = 1/2*(1+erf((beta*x+alpha)/np.sqrt(2)))\n",
    "    return probit\n",
    "\n",
    "\n",
    "def plot_dashed_lines(minimo, maximo):\n",
    "    plt.plot([0, 0], [0, 1], '--k', lw=0.2)\n",
    "    plt.plot([minimo, maximo], [0.5, 0.5], '--k', lw=0.2)\n",
    "    \n",
    "\n",
    "def plot_trials(folder, num_steps, num_trials):\n",
    "    num_rows = 6\n",
    "    for ind_sp in range(num_rows):\n",
    "        plt.subplot(num_rows, 1, ind_sp+1)\n",
    "        # remove all previous plots\n",
    "        rm_lines()\n",
    "    data = np.load(folder + '/all_points_' + str(num_trials) + '.npz')\n",
    "    new_tr_flag = data['new_trial_flags']\n",
    "    # plot the stimulus\n",
    "    plt.subplot(num_rows, 1, 1)\n",
    "    states = data['states']\n",
    "    shape_aux = (states.shape[0], states.shape[2])\n",
    "    states = np.reshape(states, shape_aux)[0:num_steps, :]\n",
    "    plt.imshow(states[:, 0:2].T, aspect='auto', cmap='gray')\n",
    "    minimo = -0.5\n",
    "    maximo = 1.5\n",
    "    plot_trials_start(new_tr_flag, minimo, maximo, num_steps, color='y')\n",
    "\n",
    "    plt.ylabel('stim')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    # go over trials and compute cumulative evidence\n",
    "    trials = np.nonzero(new_tr_flag)[0]\n",
    "    trials = np.concatenate((np.array([-1]), trials))\n",
    "    plt.subplot(num_rows, 1, 2)\n",
    "    evidence = np.zeros((num_steps,))\n",
    "    for ind_time in range(num_steps):\n",
    "        if ind_time in trials:\n",
    "            # the cumulative evidence is 0 in the beginning\n",
    "            evidence[ind_time] = 0\n",
    "        else:\n",
    "            # 0 belongs to trials so this is always valid\n",
    "            previous_evidence = evidence[ind_time-1]\n",
    "            evidence[ind_time] = previous_evidence +\\\n",
    "                (states[ind_time, 0]-states[ind_time, 1])\n",
    "\n",
    "    # plot evidence\n",
    "    plt.plot(evidence)\n",
    "    plt.plot([0, num_steps], [0, 0], '--k', lw=0.5)\n",
    "    plt.xlim(-0.5, 99.5)\n",
    "    plot_trials_start(new_tr_flag,\n",
    "                      np.min(evidence), np.max(evidence), num_steps)\n",
    "    plt.ylabel('evidence')\n",
    "    plt.xticks([])\n",
    "    plt.plot(new_tr_flag)\n",
    "    # plot actions\n",
    "    actions = data['actions']\n",
    "    actions = np.reshape(actions, (1, -1))\n",
    "    minimo = -0.5\n",
    "    maximo = 0.5\n",
    "    plt.subplot(num_rows, 1, 3)\n",
    "    plt.imshow(actions, aspect='auto', cmap='viridis')\n",
    "    plot_trials_start(new_tr_flag, minimo, maximo, num_steps, color='w')\n",
    "    plt.ylabel('action')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    # plot the rewards\n",
    "    rewards = data['rewards']\n",
    "    rewards = np.reshape(rewards, (1, -1))\n",
    "    minimo = -0.5\n",
    "    maximo = 0.5\n",
    "    plt.subplot(num_rows, 1, 4)\n",
    "    plt.imshow(rewards, aspect='auto', cmap='jet')\n",
    "    plot_trials_start(new_tr_flag, minimo, maximo, num_steps, color='w')\n",
    "    plt.ylabel('reward')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    # plot the performance\n",
    "    performance = np.array(data['corrects'])[0:num_steps, :]\n",
    "    performance = performance.T\n",
    "    minimo = -0.5\n",
    "    maximo = 0.5\n",
    "    plt.subplot(num_rows, 1, 5)\n",
    "    plt.imshow(performance, aspect='auto', cmap='jet')\n",
    "    plot_trials_start(new_tr_flag, minimo, maximo, num_steps, color='w')\n",
    "    plt.ylabel('correct')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    # plot the ground truth\n",
    "    plt.subplot(num_rows, 1, 6)\n",
    "    states = data['stims_conf']==1.0\n",
    "    states = states[0:num_steps, :]\n",
    "    plt.imshow(states[:, 0:2].T, aspect='auto', cmap='gray')\n",
    "    minimo = -0.5\n",
    "    maximo = 1.5\n",
    "    plot_trials_start(new_tr_flag, minimo, maximo, num_steps, color='y')\n",
    "\n",
    "    plt.ylabel('gr. truth')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    \n",
    "    # plot neurons' activities\n",
    "    if len(data['net_state']) != 0:\n",
    "        activity = data['net_state']\n",
    "        plt.subplot(num_rows, 1, 6)\n",
    "        shape_aux = (activity.shape[0], activity.shape[2])\n",
    "        activity = np.reshape(activity, shape_aux)[0:num_steps, :]\n",
    "        maximo = np.max(activity, axis=0).reshape(1, activity.shape[1])\n",
    "        activity /= maximo\n",
    "        activity[np.isnan(activity)] = -0.1\n",
    "        plt.imshow(activity.T, aspect='auto', cmap='hot')\n",
    "        minimo = np.min(-0.5)\n",
    "        maximo = np.max(shape_aux[1]-0.5)\n",
    "        plot_trials_start(new_tr_flag, minimo, maximo, num_steps)\n",
    "        plt.ylabel('activity')\n",
    "        plt.xlabel('time (a.u)')\n",
    "        plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true,
    "init_cell": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/molano/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "def RNN_UGRU(inputs, prev_rewards, a_size, num_units):\n",
    "\n",
    "    # create a UGRNNCell\n",
    "    rnn_cell = tf.contrib.rnn.UGRNNCell(num_units, activation=tf.nn.relu)\n",
    "\n",
    "    # this is the initial state used in the A3C model when training\n",
    "    # or obtaining an action\n",
    "    st_init = np.zeros((1, rnn_cell.state_size), np.float32)\n",
    "\n",
    "    # defining initial state\n",
    "    state_in = tf.placeholder(tf.float32, [1, rnn_cell.state_size])\n",
    "\n",
    "    # reshape inputs size\n",
    "    rnn_in = tf.expand_dims(inputs, [0])\n",
    "\n",
    "    step_size = tf.shape(prev_rewards)[:1]\n",
    "\n",
    "    # 'state' is a tensor of shape [batch_size, cell_state_size]\n",
    "    # 'outputs' is a tensor of shape [batch_size, max_time, cell_state_size]\n",
    "    outputs, state_out = tf.nn.dynamic_rnn(rnn_cell, rnn_in,\n",
    "                                           initial_state=state_in,\n",
    "                                           sequence_length=step_size,\n",
    "                                           dtype=tf.float32,\n",
    "                                           time_major=False)\n",
    "\n",
    "    rnn_out = tf.reshape(outputs, [-1, num_units])\n",
    "\n",
    "    actions, actions_onehot, policy, value = \\\n",
    "        process_output(rnn_out, outputs, a_size, num_units)\n",
    "\n",
    "    return st_init, state_in, state_out, actions, actions_onehot, policy, value\n",
    "\n",
    "def process_output(rnn_out, outputs, a_size, num_units):\n",
    "    # Actions\n",
    "    actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "    actions_onehot = tf.one_hot(actions, a_size, dtype=tf.float32)\n",
    "\n",
    "    # Output layers for policy and value estimations\n",
    "    policy = slim.fully_connected(rnn_out, a_size,\n",
    "                                  activation_fn=tf.nn.softmax,\n",
    "                                  weights_initializer=normalized_columns_initializer(0.01),\n",
    "                                  biases_initializer=None)\n",
    "    value = slim.fully_connected(rnn_out, 1,\n",
    "                                 activation_fn=None,\n",
    "                                 weights_initializer=normalized_columns_initializer(1.0),\n",
    "                                 biases_initializer=None)\n",
    "\n",
    "    return actions, actions_onehot, policy, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     16
    ],
    "hidden": true,
    "init_cell": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "class data():\n",
    "    def __init__(self, folder=''):\n",
    "        # point by point parameter mats saved for some trials\n",
    "        self.states_point = []\n",
    "        self.net_state_point = []\n",
    "        self.rewards_point = []\n",
    "        self.done_point = []\n",
    "        self.actions_point = []\n",
    "        self.corrects_point = []\n",
    "        self.new_trial_point = []\n",
    "        self.trials_point = []\n",
    "        self.stims_conf_point = []\n",
    "        # where to save the trials data\n",
    "        self.folder = folder\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        reset all mats\n",
    "        \"\"\"\n",
    "        # reset parameters mat\n",
    "        self.states_point = []\n",
    "        self.net_state_point = []\n",
    "        self.rewards_point = []\n",
    "        self.done_point = []\n",
    "        self.actions_point = []\n",
    "        self.corrects_point = []\n",
    "        self.new_trial_point = []\n",
    "        self.stims_conf_point = []\n",
    "        self.trials_point = []\n",
    "\n",
    "    def update(self, new_state=[], net_state=[], reward=None, update_net=None,\n",
    "               action=None, correct=[], new_trial=None, num_trials=None,\n",
    "               stim_conf=[]):\n",
    "        \"\"\"\n",
    "        append available info\n",
    "        \"\"\"\n",
    "        if len(new_state) != 0:\n",
    "            self.states_point.append(new_state)\n",
    "        if len(net_state) != 0:\n",
    "            self.net_state_point.append(net_state)\n",
    "        if reward is not None:\n",
    "            self.rewards_point.append(reward)\n",
    "        if update_net is not None:\n",
    "            self.done_point.append(update_net)  # 0 by construction\n",
    "        if action is not None:\n",
    "            self.actions_point.append(action)\n",
    "        if len(correct) != 0:\n",
    "            self.corrects_point.append(correct)\n",
    "        if new_trial is not None:\n",
    "            self.new_trial_point.append(new_trial)  # 0 by construction\n",
    "        if num_trials is not None:\n",
    "            self.trials_point.append(num_trials)\n",
    "        if len(stim_conf) != 0:\n",
    "            self.stims_conf_point.append(stim_conf)\n",
    "\n",
    "    def save(self, num_trials):\n",
    "        \"\"\"\n",
    "        save data\n",
    "        \"\"\"\n",
    "        data = {'states': self.states_point, 'net_state': self.net_state_point,\n",
    "                'rewards': self.rewards_point, 'done_flags': self.done_point,\n",
    "                'actions': self.actions_point, 'corrects': self.corrects_point,\n",
    "                'new_trial_flags': self.new_trial_point,\n",
    "                'trials_saved': self.trials_point,\n",
    "                'stims_conf': self.stims_conf_point}\n",
    "        np.savez(self.folder + '/all_points_' + str(num_trials) + '.npz',\n",
    "                 **data)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "init_cell": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class PriorsEnv():\n",
    "    metadata = {}\n",
    "\n",
    "    def __init__(self, trial_dur=10, upd_net=5,rep_prob=(.2, .8), rewards=(0.1, -0.1, 1.0, -1.0),\n",
    "                 env_seed='0', block_dur=200, stim_ev=0.5, folder=None,\n",
    "                 plot=False):\n",
    "        # num steps per trial\n",
    "        self.trial_dur = trial_dur\n",
    "        # rewards given for: stop fixating, keep fixating, correct, wrong\n",
    "        self.rewards = rewards\n",
    "        # number of trials per blocks\n",
    "        self.block_dur = block_dur\n",
    "        # stimulus evidence: one stimulus is always N(1,1), the mean of\n",
    "        # the other is drawn from a uniform distrib.=U(stim_ev,1).\n",
    "        # stim_ev must then be between 0 and 1 and the higher it is\n",
    "        # the more difficult will be the task\n",
    "        self.stim_ev = stim_ev\n",
    "        # prob. of repeating the stimuli in the positions of previous trial\n",
    "        self.rep_prob = rep_prob\n",
    "        # model instance\n",
    "        self.env_seed = env_seed\n",
    "        # folder to save data\n",
    "        self.folder = folder\n",
    "        # update parameters\n",
    "        self.upd_net = upd_net\n",
    "\n",
    "        # num actions\n",
    "        self.num_actions = 3\n",
    "\n",
    "        # position of the first stimulus\n",
    "        self.stms_pos_new_trial = np.random.choice([0, 1])\n",
    "        # keeps track of the repeating prob of the current block\n",
    "        self.curr_rep_prob = np.random.choice([0, 1])\n",
    "        # position of the stimuli\n",
    "        self.stm_pos_new_trial = 0\n",
    "        # steps counter\n",
    "        self.timestep = 0\n",
    "        # initialize ground truth state [stim1 mean, stim2 mean, fixation])\n",
    "        # the network has to output the action corresponding to the stim1 mean\n",
    "        # that will be always 1.0 (I just initialize here at 0 for convinience)\n",
    "        self.int_st = np.array([0, 0, -1])\n",
    "        # accumulated evidence\n",
    "        self.evidence = 0\n",
    "        # number of trials\n",
    "        self.num_tr = 0\n",
    "\n",
    "        # trial data to save\n",
    "        # stimulus evidence\n",
    "        self.ev_mat = []\n",
    "        # position of stimulus 1\n",
    "        self.stm_pos = []\n",
    "        # performance\n",
    "        self.perf_mat = []\n",
    "        # summed activity across the trial\n",
    "        self.action = []\n",
    "\n",
    "        # point by point parameter mats saved for some trials\n",
    "        self.all_pts_data = data(folder=folder)\n",
    "\n",
    "        # save all points step. Here I call the class data that implements\n",
    "        # all the necessary functions\n",
    "        self.sv_pts_stp = 10\n",
    "        self.num_tr_svd = 1000\n",
    "\n",
    "        # figures for plotting\n",
    "        self.plot_figs = plot\n",
    "        if self.plot_figs:\n",
    "            self.perf_fig = plt.figure(figsize=(8, 8), dpi=100)\n",
    "            self.bias_fig = plt.figure(figsize=(8, 8), dpi=100)\n",
    "            self.trials_fig = plt.figure(figsize=(8, 8), dpi=100)\n",
    "\n",
    "            print('--------------- Priors experiment ---------------')\n",
    "            print('Update of networks (in trials): ' +\n",
    "                  str(self.upd_net))\n",
    "            print('Duration of each trial (in steps): ' + str(self.trial_dur))\n",
    "            print('Rewards: ' + str(self.rewards))\n",
    "            print('Duration of each block (in trials): ' + str(self.block_dur))\n",
    "            print('Repeating probabilities of each block: ' + str(self.rep_prob))\n",
    "            print('Stim evidence: ' + str(self.stim_ev))\n",
    "            print('Saving folder: ' + str(self.folder))\n",
    "            print('--------------- ----------------- ---------------')\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        receives an action and returns a reward, a state and flag variables\n",
    "        that indicate whether to start a new trial and whether to update\n",
    "        the network\n",
    "        \"\"\"\n",
    "        new_trial = True\n",
    "        correct = False\n",
    "        done = False\n",
    "        # decide which reward and state (new_trial, correct) we are in\n",
    "        if self.timestep < self.trial_dur:\n",
    "            if (self.int_st[action] != -1).all():\n",
    "                reward = self.rewards[0]\n",
    "            else:\n",
    "                # don't abort the trial even if the network stops fixating\n",
    "                reward = self.rewards[1]\n",
    "\n",
    "            new_trial = False\n",
    "\n",
    "        else:\n",
    "            if (self.int_st[action] == 1.0).all():\n",
    "                reward = self.rewards[2]\n",
    "                correct = True\n",
    "            else:\n",
    "                reward = self.rewards[3]\n",
    "\n",
    "        if new_trial:\n",
    "            # keep main variables of the trial\n",
    "            self.stm_pos.append(self.stms_pos_new_trial)\n",
    "            self.perf_mat.append(correct)\n",
    "            self.action.append(action)\n",
    "            self.ev_mat.append(self.evidence)\n",
    "            new_st = self.new_trial()\n",
    "            # check if it is time to update the network\n",
    "            done = ((self.num_tr-1) % self.upd_net == 0) and (self.num_tr != 1)\n",
    "            # check if it is time to save the trial-to-trial data\n",
    "            if self.num_tr % 10000 == 0:\n",
    "                self.save_trials_data()\n",
    "                if self.plot_figs:\n",
    "                    self.output_stats()\n",
    "\n",
    "            # point by point parameter mats saved for some periods\n",
    "            if np.floor(self.num_tr / self.num_tr_svd) % self.sv_pts_stp == 0:\n",
    "                self.all_pts_data.update(reward=reward,\n",
    "                                         update_net=done,\n",
    "                                         action=action, correct=[correct])\n",
    "\n",
    "            # during some episodes I save all data points\n",
    "            aux = np.floor((self.num_tr-1) / self.num_tr_svd)\n",
    "            aux2 = np.floor(self.num_tr / self.num_tr_svd)\n",
    "            if aux % self.sv_pts_stp == 0 and\\\n",
    "               aux2 % self.sv_pts_stp == 1:\n",
    "                self.all_pts_data.save(self.num_tr)\n",
    "                self.all_pts_data.reset()\n",
    "                if self.plot_figs:\n",
    "                    plt.figure(self.trials_fig.number)\n",
    "                    plot_trials(self.folder, 100, self.num_tr)\n",
    "                    self.trials_fig.canvas.draw()\n",
    "\n",
    "        else:\n",
    "            new_st = self.get_state()\n",
    "            # during some episodes I save all data points\n",
    "            if np.floor(self.num_tr / self.num_tr_svd) % self.sv_pts_stp == 0:\n",
    "                self.all_pts_data.update(new_state=new_st,\n",
    "                                         reward=reward, update_net=done,\n",
    "                                         action=action, correct=[correct],\n",
    "                                         new_trial=new_trial,\n",
    "                                         num_trials=self.num_tr,\n",
    "                                         stim_conf=self.int_st)\n",
    "\n",
    "        return new_st, reward, done, new_trial\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Outputs a new observation using stim 1 and 2 means.\n",
    "        It also outputs a fixation signal that is always -1 except at the\n",
    "        end of the trial that is 0\n",
    "        \"\"\"\n",
    "        self.timestep += 1\n",
    "        # if still in the integration period present a new observation\n",
    "        if self.timestep < self.trial_dur:\n",
    "            self.state = [np.random.normal(self.int_st[0]),\n",
    "                          np.random.normal(self.int_st[1]), -1]\n",
    "        else:\n",
    "            self.state = [0, 0, 0]\n",
    "\n",
    "        # update evidence\n",
    "        self.evidence += self.state[0]-self.state[1]\n",
    "\n",
    "        return np.reshape(self.state, [1, self.num_actions, 1])\n",
    "\n",
    "    def new_trial(self):\n",
    "        \"\"\"\n",
    "        this function creates a new trial, deciding the amount of coherence\n",
    "        (through the mean of stim 2) and the position of stim 1. Once it has\n",
    "        done this it calls get_state to get the first observation of the trial\n",
    "        \"\"\"\n",
    "        self.num_tr += 1\n",
    "        self.timestep = 0\n",
    "        self.evidence = 0\n",
    "        # this are the means of the two stimuli\n",
    "        stim1 = 1.0\n",
    "        stim2 = np.random.uniform(1-self.stim_ev, 1)\n",
    "        assert stim2 != 1.0\n",
    "        self.choices = [stim1, stim2]\n",
    "\n",
    "        # decide the position of the stims\n",
    "        # if the block is finished update the prob of repeating\n",
    "        if self.num_tr % self.block_dur == 0:\n",
    "            self.curr_rep_prob = int(not self.curr_rep_prob)\n",
    "\n",
    "        # flip a coin\n",
    "        repeat = np.random.uniform() < self.rep_prob[self.curr_rep_prob]\n",
    "        if not repeat:\n",
    "            self.stms_pos_new_trial = not(self.stms_pos_new_trial)\n",
    "\n",
    "        aux = [self.choices[x] for x in [int(self.stms_pos_new_trial),\n",
    "                                         int(not self.stms_pos_new_trial)]]\n",
    "\n",
    "        self.int_st = np.concatenate((aux, np.array([-1])))\n",
    "\n",
    "        # get state\n",
    "        s = self.get_state()\n",
    "\n",
    "        # during some episodes I save all data points\n",
    "        if np.floor(self.num_tr/self.num_tr_svd) % self.sv_pts_stp == 0:\n",
    "            self.all_pts_data.update(new_state=s,\n",
    "                                     new_trial=1,\n",
    "                                     num_trials=self.num_tr,\n",
    "                                     stim_conf=self.int_st)\n",
    "\n",
    "        return s\n",
    "\n",
    "    def save_trials_data(self):\n",
    "        \"\"\"\n",
    "        save trial-to-trial data for:\n",
    "        evidence, stim postion, action taken and outcome\n",
    "        \"\"\"\n",
    "        # Periodically save model trials statistics.\n",
    "        data = {'stims_position': self.stm_pos,\n",
    "                'action': self.action,\n",
    "                'performance': self.perf_mat,\n",
    "                'evidence': self.ev_mat}\n",
    "        np.savez(self.folder + '/trials_stats_' +\n",
    "                 str(self.env_seed) + '_' + str(self.num_tr) + '.npz', **data)\n",
    "\n",
    "    def reset(self):\n",
    "        return self.new_trial()\n",
    "\n",
    "    def output_stats(self):\n",
    "        \"\"\"\n",
    "        plot temporary learning and bias curves\n",
    "        \"\"\"\n",
    "        aux_shape = (1, len(self.ev_mat))\n",
    "        # plot psycho. curves\n",
    "        plt.figure(self.bias_fig.number)\n",
    "        per = 50000\n",
    "        ev = self.ev_mat.copy()\n",
    "        ev = np.reshape(ev, aux_shape)[np.max([0, len(ev)-per]):]\n",
    "        perf = self.perf_mat.copy()\n",
    "        perf = np.reshape(perf,aux_shape)[np.max([0, len(perf)-per]):]\n",
    "        action = self.action.copy()\n",
    "        action = np.reshape(action, aux_shape)[np.max([0, len(action)-per]):]\n",
    "        plot_psychometric_curves(ev, perf, action, blk_dur=self.block_dur)\n",
    "        self.bias_fig.canvas.draw()\n",
    "        # plot learning\n",
    "        plt.figure(self.perf_fig.number)\n",
    "        ev = self.ev_mat.copy()\n",
    "        ev = np.reshape(ev, aux_shape)[np.max([0, len(ev)-per]):]\n",
    "        perf = self.perf_mat.copy()\n",
    "        perf = np.reshape(perf,aux_shape)[np.max([0, len(perf)-per]):]\n",
    "        action = self.action.copy()\n",
    "        action = np.reshape(action, aux_shape)[np.max([0, len(action)-per]):]\n",
    "        stim_pos = self.stm_pos.copy()\n",
    "        stim_pos = np.reshape(stim_pos, aux_shape)[np.max([0, len(stim_pos)-per]):]\n",
    "        plot_learning(perf, ev, stim_pos, action)\n",
    "        self.perf_fig.canvas.draw()\n",
    "        # plot trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true,
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# np.random.seed(0)\n",
    "# env = PriorsEnv(exp_dur=10**7, trial_dur=10, upd_net=5, rep_prob=(.2, .8), rewards=(-0.1, 0.0, 1.0, -1.0),\n",
    "#                 env_seed='0', block_dur=200, stim_ev=0.99,\n",
    "#                 folder='/home/molano/priors_project/priors/test', plot=False)\n",
    "# env.new_trial()\n",
    "# env.step(0)\n",
    "# env.step(0)\n",
    "# print(env.all_pts_data.stims_conf_point)\n",
    "# for ind in range(1000):\n",
    "#     new_st, reward, done, new_trial = env.step(1*(env.evidence>0))\n",
    "#     print(env.timestep)\n",
    "#     if new_trial:\n",
    "#         print('---------')\n",
    "#     if (ind+1) % 100 == 0:\n",
    "#         env.all_pts_data.save(int(ind/100))\n",
    "#         plt.figure(env.trials_fig.number)\n",
    "#         plt.plot(np.arange(10))\n",
    "#         self.trials_fig.canvas.draw()\n",
    "#         asdasd\n",
    "#         env.all_pts_data.plot(100, int(ind/100))\n",
    "# assert 0   \n",
    "#     if new_trial:\n",
    "#         print(new_st[0].T)\n",
    "#         print(reward)\n",
    "#         print(env.int_st)\n",
    "#         print(env.evidence)\n",
    "#         print('-----------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "init_cell": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class AC_Network():\n",
    "    def __init__(self, a_size, state_size, scope, trainer, num_units, network):\n",
    "        with tf.variable_scope(scope):\n",
    "            # Input and visual encoding layers\n",
    "            self.st = tf.placeholder(shape=[None, 1, state_size, 1],\n",
    "                                     dtype=tf.float32)\n",
    "            self.prev_rewards = tf.placeholder(shape=[None, 1],\n",
    "                                               dtype=tf.float32)\n",
    "            self.prev_actions = tf.placeholder(shape=[None],\n",
    "                                               dtype=tf.int32)\n",
    "\n",
    "            self.prev_actions_onehot = tf.one_hot(self.prev_actions, a_size,\n",
    "                                                  dtype=tf.float32)\n",
    "\n",
    "            hidden = tf.concat([slim.flatten(self.st), self.prev_rewards,\n",
    "                                self.prev_actions_onehot], 1)\n",
    "\n",
    "            # call RNN network\n",
    "            if network == 'relu':\n",
    "                net = RNN_ReLU\n",
    "            elif network == 'lstm':\n",
    "                net = RNN\n",
    "            elif network == 'gru':\n",
    "                net = RNN_GRU\n",
    "            elif network == 'ugru':\n",
    "                net = RNN_UGRU\n",
    "            else:\n",
    "                raise ValueError('Unknown network')\n",
    "\n",
    "            self.st_init, self.st_in, self.st_out, self.actions,\\\n",
    "                self.actions_onehot, self.policy, self.value =\\\n",
    "                net(hidden, self.prev_rewards, a_size, num_units)\n",
    "\n",
    "            # Only the worker network needs ops for loss functions\n",
    "            # and gradient updating.\n",
    "            if scope != 'global':\n",
    "                self.target_v = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "                self.advantages = tf.placeholder(shape=[None],\n",
    "                                                 dtype=tf.float32)\n",
    "\n",
    "                self.resp_outputs = \\\n",
    "                    tf.reduce_sum(self.policy * self.actions_onehot, [1])\n",
    "\n",
    "                # Loss functions\n",
    "                self.value_loss = 0.5 * tf.reduce_sum(\n",
    "                    tf.square(self.target_v -\n",
    "                              tf.reshape(self.value, [-1])))\n",
    "                self.entropy = - tf.reduce_sum(\n",
    "                    self.policy * tf.log(self.policy + 1e-7))\n",
    "                self.policy_loss = -tf.reduce_sum(\n",
    "                    tf.log(self.resp_outputs + 1e-7)*self.advantages)\n",
    "                self.loss = 0.5 * self.value_loss +\\\n",
    "                    self.policy_loss -\\\n",
    "                    self.entropy * 0.05\n",
    "\n",
    "                # Get gradients from local network using local losses\n",
    "                local_vars = tf.get_collection(\n",
    "                    tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "                self.gradients = tf.gradients(self.loss, local_vars)\n",
    "                self.var_norms = tf.global_norm(local_vars)\n",
    "                grads, self.grad_norms =\\\n",
    "                    tf.clip_by_global_norm(self.gradients, 999.0)\n",
    "\n",
    "                # Apply local gradients to global network\n",
    "                global_vars = tf.get_collection(\n",
    "                    tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "                self.apply_grads = trainer.apply_gradients(\n",
    "                    zip(grads, global_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Worker class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "init_cell": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "    def __init__(self, game, name, a_size, state_size, trainer,\n",
    "                 model_path, global_epss, data_path, num_units, network):\n",
    "        self.name = \"worker_\" + str(name)\n",
    "        self.number = name\n",
    "        self.folder = data_path + '/trains/train_' + str(self.number)\n",
    "        self.model_path = model_path\n",
    "        self.trainer = trainer\n",
    "        self.global_epss = global_epss\n",
    "        self.increment = self.global_epss.assign_add(1)\n",
    "        self.network = network\n",
    "        self.eps_rewards = []\n",
    "        self.eps_mean_values = []\n",
    "\n",
    "        self.summary_writer = tf.summary.FileWriter(self.folder)\n",
    "\n",
    "        # Create the local copy of the network and the tensorflow op\n",
    "        # to copy global parameters to local network\n",
    "        self.local_AC = AC_Network(a_size, state_size, self.name, trainer,\n",
    "                                   num_units, network)\n",
    "        self.update_local_ops = update_target_graph('global', self.name)\n",
    "        self.env = game\n",
    "\n",
    "    def train(self, rollout, sess, gamma, bootstrap_value):\n",
    "        rollout = np.array(rollout)\n",
    "        states = rollout[:, 0]\n",
    "        actions = rollout[:, 1]\n",
    "        rewards = rollout[:, 2]\n",
    "\n",
    "        prev_rewards = [0] + rewards[:-1].tolist()\n",
    "        prev_actions = [0] + actions[:-1].tolist()\n",
    "        values = rollout[:, 3]\n",
    "\n",
    "        self.pr = prev_rewards\n",
    "        self.pa = prev_actions\n",
    "        # Here we take the rewards and values from the rollout, and use them to\n",
    "        # generate the advantage and discounted returns.\n",
    "        # The advantage function uses \"Generalized Advantage Estimation\"\n",
    "        self.rewards_plus = np.asarray(rewards.tolist() + [bootstrap_value])\n",
    "        discounted_rewards = discount(self.rewards_plus, gamma)[:-1]\n",
    "        self.value_plus = np.asarray(values.tolist() + [bootstrap_value])\n",
    "        advantages = rewards +\\\n",
    "            gamma * self.value_plus[1:] -\\\n",
    "            self.value_plus[:-1]\n",
    "        advantages = discount(advantages, gamma)\n",
    "\n",
    "        # Update the global network using gradients from loss\n",
    "        # Generate network statistics to periodically save\n",
    "        rnn_state = self.local_AC.st_init\n",
    "        if self.network == 'lstm':\n",
    "            feed_dict = {self.local_AC.target_v: discounted_rewards,\n",
    "                         self.local_AC.state: np.stack(states, axis=0),\n",
    "                         self.local_AC.prev_rewards: np.vstack(prev_rewards),\n",
    "                         self.local_AC.prev_actions: prev_actions,\n",
    "                         self.local_AC.actions: actions,\n",
    "                         self.local_AC.advantages: advantages,\n",
    "                         self.local_AC.state_in[0]: rnn_state[0],\n",
    "                         self.local_AC.state_in[1]: rnn_state[1]}\n",
    "        elif (self.network == 'relu') or\\\n",
    "             (self.network == 'gru') or\\\n",
    "             (self.network == 'ugru'):\n",
    "            feed_dict = {self.local_AC.target_v: discounted_rewards,\n",
    "                         self.local_AC.st: np.stack(states, axis=0),\n",
    "                         self.local_AC.prev_rewards: np.vstack(prev_rewards),\n",
    "                         self.local_AC.prev_actions: prev_actions,\n",
    "                         self.local_AC.actions: actions,\n",
    "                         self.local_AC.advantages: advantages,\n",
    "                         self.local_AC.st_in: rnn_state}\n",
    "\n",
    "        v_l, p_l, e_l, g_n, v_n, _ = sess.run([self.local_AC.value_loss,\n",
    "                                               self.local_AC.policy_loss,\n",
    "                                               self.local_AC.entropy,\n",
    "                                               self.local_AC.grad_norms,\n",
    "                                               self.local_AC.var_norms,\n",
    "                                               self.local_AC.apply_grads],\n",
    "                                              feed_dict=feed_dict)\n",
    "        aux = len(rollout)\n",
    "        return v_l / aux, p_l / aux, e_l / aux, g_n, v_n\n",
    "\n",
    "    def work(self, gamma, sess, coord, saver, train, exp_dur):\n",
    "        eps_count = sess.run(self.global_epss)\n",
    "        num_eps_tr_stats = int(1000/self.env.upd_net)\n",
    "        num_epss_end = int(exp_dur/self.env.upd_net)\n",
    "        num_epss_save_model = int(5000/self.env.upd_net)\n",
    "        total_steps = 0\n",
    "        print(\"Starting worker \" + str(self.number))\n",
    "        # get first state\n",
    "        s = self.env.new_trial()\n",
    "        with sess.as_default(), sess.graph.as_default():\n",
    "            while not coord.should_stop():\n",
    "                sess.run(self.update_local_ops)\n",
    "                eps_buffer = []\n",
    "                eps_values = []\n",
    "                eps_reward = 0\n",
    "                eps_step_count = 0\n",
    "                d = False\n",
    "                r = 0\n",
    "                a = 0\n",
    "                rnn_state = self.local_AC.st_init\n",
    "                while not d:\n",
    "                    if self.network == 'lstm':\n",
    "                        feed_dict = {\n",
    "                                    self.local_AC.state: [s],\n",
    "                                    self.local_AC.prev_rewards: [[r]],\n",
    "                                    self.local_AC.prev_actions: [a],\n",
    "                                    self.local_AC.state_in[0]: rnn_state[0],\n",
    "                                    self.local_AC.state_in[1]: rnn_state[1]}\n",
    "                    elif (self.network == 'relu') or\\\n",
    "                         (self.network == 'gru') or\\\n",
    "                         (self.network == 'ugru'):\n",
    "                        feed_dict = {\n",
    "                                    self.local_AC.st: [s],\n",
    "                                    self.local_AC.prev_rewards: [[r]],\n",
    "                                    self.local_AC.prev_actions: [a],\n",
    "                                    self.local_AC.st_in: rnn_state}\n",
    "\n",
    "                    # Take an action using probs from policy network output\n",
    "                    a_dist, v, rnn_state_new = sess.run(\n",
    "                                                        [self.local_AC.policy,\n",
    "                                                         self.local_AC.value,\n",
    "                                                         self.local_AC.st_out],\n",
    "                                                        feed_dict=feed_dict)\n",
    "\n",
    "                    a = np.random.choice(a_dist[0], p=a_dist[0])\n",
    "                    a = np.argmax(a_dist == a)\n",
    "                    rnn_state = rnn_state_new\n",
    "                    # new_state, reward, update_net, new_trial\n",
    "                    s1, r, d, nt = self.env.step(a)\n",
    "                    # save samples for training the network later\n",
    "                    eps_buffer.append([s, a, r, v[0, 0]])\n",
    "                    eps_values.append(v[0, 0])\n",
    "                    eps_reward += r\n",
    "                    total_steps += 1\n",
    "                    eps_step_count += 1\n",
    "                    s = s1\n",
    "\n",
    "                self.eps_rewards.append(eps_reward)\n",
    "                self.eps_mean_values.append(np.mean(eps_values))\n",
    "\n",
    "                # Update the network using the experience buffer\n",
    "                # at the end of the episode\n",
    "                if len(eps_buffer) != 0 and train:\n",
    "                    v_l, p_l, e_l, g_n, v_n = \\\n",
    "                        self.train(eps_buffer, sess, gamma, 0.0)\n",
    "\n",
    "                # Periodically save model parameters and summary statistics.\n",
    "                if eps_count % num_eps_tr_stats == 0 and eps_count != 0:\n",
    "                    if eps_count % num_epss_save_model == 0 and\\\n",
    "                       self.name == 'worker_0' and\\\n",
    "                       train and\\\n",
    "                       len(self.eps_rewards) != 0:\n",
    "                        saver.save(sess, self.model_path +\n",
    "                                   '/model-' + str(eps_count) + '.cptk')\n",
    "                    mean_reward = np.mean(self.eps_rewards[-10:])\n",
    "                    mean_value = np.mean(self.eps_mean_values[-10:])\n",
    "                    summary = tf.Summary()\n",
    "                    summary.value.add(tag='Perf/Reward',\n",
    "                                      simple_value=float(mean_reward))\n",
    "                    summary.value.add(tag='Perf/Value',\n",
    "                                      simple_value=float(mean_value))\n",
    "\n",
    "                    performance_aux = np.vstack(np.array(self.env.perf_mat))\n",
    "\n",
    "                    for ind_crr in range(performance_aux.shape[1]):\n",
    "                        mean_performance = np.mean(performance_aux[:, ind_crr])\n",
    "                        summary.value.add(tag='Perf/Perf_' + str(ind_crr),\n",
    "                                          simple_value=float(mean_performance))\n",
    "\n",
    "                    if train:\n",
    "                        summary.value.add(tag='Losses/Value Loss',\n",
    "                                          simple_value=float(v_l))\n",
    "                        summary.value.add(tag='Losses/Policy Loss',\n",
    "                                          simple_value=float(p_l))\n",
    "                        summary.value.add(tag='Losses/Entropy',\n",
    "                                          simple_value=float(e_l))\n",
    "                        summary.value.add(tag='Losses/Grad Norm',\n",
    "                                          simple_value=float(g_n))\n",
    "                        summary.value.add(tag='Losses/Var Norm',\n",
    "                                          simple_value=float(v_n))\n",
    "                    self.summary_writer.add_summary(summary, eps_count)\n",
    "\n",
    "                    self.summary_writer.flush()\n",
    "\n",
    "                if self.name == 'worker_0':\n",
    "                    sess.run(self.increment)\n",
    "\n",
    "                eps_count += 1\n",
    "                if eps_count > num_epss_end:\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Call the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "init_cell": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "def main_priors(load_model=False, train=True, gamma=.8, up_net=5,\n",
    "                trial_dur=10, rep_prob=(0.2, 0.8), exp_dur=1000,\n",
    "                rewards=(-0.1, 0.0, 1.0, -1.0), block_dur=200,\n",
    "                num_units=32, stim_ev=.3, network='ugru',\n",
    "                learning_rate=1e-3, instance=0, main_folder=''):\n",
    "    a_size = 3  # number of actions\n",
    "    state_size = a_size  # number of inputs\n",
    "    if train:\n",
    "        test_flag = ''\n",
    "    else:\n",
    "        test_flag = '_test'\n",
    "    data_path = folder_name(gamma=gamma, up_net=up_net, trial_dur=trial_dur,\n",
    "                rep_prob=rep_prob, exp_dur=exp_dur, rewards=rewards,\n",
    "                block_dur=block_dur, num_units=num_units,\n",
    "                stim_ev=stim_ev, network=net, learning_rate=lr,\n",
    "                instance=inst, main_folder=main_folder, t_flag=test_flag)\n",
    "\n",
    "    data = {'trial_dur': trial_dur, 'rep_prob': rep_prob,\n",
    "            'rewards': rewards, 'stim_ev': stim_ev,\n",
    "            'block_dur': block_dur, 'gamma': gamma, 'num_units': num_units,\n",
    "            'up_net': up_net, 'network': network}\n",
    "\n",
    "    model_path = data_path + '/model_meta_context'\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    np.savez(data_path + '/experiment_setup.npz', **data)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        global_episodes = tf.Variable(0, dtype=tf.int32,\n",
    "                                      name='global_episodes',\n",
    "                                      trainable=False)\n",
    "        trainer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        AC_Network(a_size, state_size, 'global',\n",
    "                             None, num_units, network)  # Generate global net\n",
    "        # Set workers to number of available CPU threads\n",
    "        num_workers = multiprocessing.cpu_count()\n",
    "        workers = []\n",
    "        # Create worker classes\n",
    "        for i in range(num_workers):\n",
    "            saving_path = data_path + '/trains/train_' + str(i)\n",
    "            if not os.path.exists(saving_path):\n",
    "                os.makedirs(saving_path)\n",
    "            workers.append(Worker(PriorsEnv(upd_net=up_net, trial_dur=trial_dur,\n",
    "                              rep_prob=rep_prob, rewards=rewards,\n",
    "                              block_dur=block_dur, stim_ev=stim_ev,\n",
    "                              folder=saving_path, plot=(i==0)), i, a_size, state_size,\n",
    "                            trainer, model_path, global_episodes,\n",
    "                            data_path, num_units, network))\n",
    "        saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        coord = tf.train.Coordinator()\n",
    "        if load_model:\n",
    "            print('Loading Model...')\n",
    "            print(model_path)\n",
    "            ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        worker_threads = []\n",
    "        for worker in workers:\n",
    "            worker_work = lambda: worker.work(gamma, sess, coord, saver, train, exp_dur)\n",
    "            thread = threading.Thread(target=(worker_work))\n",
    "            thread.start()\n",
    "            worker_threads.append(thread)\n",
    "        coord.join(worker_threads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hide_output": false,
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# experiment duration \n",
    "exp_dur = 10**6\n",
    "# num steps per trial\n",
    "trial_dur = 10\n",
    "# rewards given for: stop fixating, keep fixating, correct, wrong\n",
    "rewards = (-0.1, 0.0, 1.0, -1.0)\n",
    "# number of trials per blocks\n",
    "block_dur = 200\n",
    "# stimulus evidence\n",
    "stim_ev = 0.5\n",
    "# prob. of repeating the stimuli in the positions of previous trial\n",
    "rep_prob = [0.2, 0.8]\n",
    "# discount factor\n",
    "gamma = 0.8\n",
    "# learning rate\n",
    "lr = 1e-3\n",
    "# num units in the network\n",
    "num_units = 32\n",
    "# trials to updated the network weights\n",
    "up_net = 5\n",
    "# network units\n",
    "net = 'ugru'\n",
    "# instance\n",
    "inst = 123\n",
    "# folder where data will be saved\n",
    "main_folder = '/home/molano/priors_project/priors/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_priors(load_model=False, train=True, gamma=gamma, up_net=up_net,\n",
    "            trial_dur=trial_dur, rep_prob=rep_prob, exp_dur=exp_dur,\n",
    "            rewards=rewards, block_dur=block_dur,\n",
    "            num_units=num_units, stim_ev=stim_ev, network=net,\n",
    "            learning_rate=lr, instance=inst, main_folder=main_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hide_output": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_flag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f15f70a5acfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mstim_ev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ugru'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10e-3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 instance=0, main_folder='', t_flag='') + '/trains/train_' +\\\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/trials_stats_0_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_tr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-3fde87dfe7c1>\u001b[0m in \u001b[0;36mfolder_name\u001b[0;34m(gamma, up_net, trial_dur, rep_prob, exp_dur, rewards, block_dur, num_units, stim_ev, network, learning_rate, instance, main_folder, t_flag)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mstim_ev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ugru'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10e-3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 instance=0, main_folder='', t_flag=''):\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmain_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/td_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial_dur\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_rp_'\u001b[0m \u001b[0;34m+\u001b[0m        \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_r_'\u001b[0m \u001b[0;34m+\u001b[0m        \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_bd_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_dur\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m        \u001b[0;34m'_ev_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstim_ev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_g_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_lr_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m        \u001b[0;34m'_nu_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_un_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mup_net\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m        \u001b[0;34m'_net_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_ed_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_dur\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_flag\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_flag' is not defined"
     ]
    }
   ],
   "source": [
    "# num trials file\n",
    "num_tr = 980000\n",
    "# worker\n",
    "worker = 0\n",
    "exp = folder_name(gamma=0.8, up_net=5, trial_dur=10,\n",
    "                rep_prob=(.2, .8), exp_dur=10**6, rewards=(-0.1, 0.0, 1.0, -1.0),\n",
    "                block_dur=200, num_units=32,\n",
    "                stim_ev=0.5, network='ugru', learning_rate=10e-3,\n",
    "                instance=0, main_folder='', t_flag='') + '/trains/train_' +\\\n",
    "        str(worker) + '/trials_stats_0_' + str(num_tr) + '.npz'\n",
    "data = np.load(exp)\n",
    "\n",
    "ev = np.reshape(data['evidence'], (1, data['evidence'].shape[0])).copy()\n",
    "perf = np.reshape(data['performance'],\n",
    "                  (1, data['performance'].shape[0])).copy()\n",
    "action = np.reshape(data['action'], (1, data['action'].shape[0])).copy()\n",
    "stim_pos = np.reshape(data['stims_position'],\n",
    "                      (1, data['stims_position'].shape[0])).copy()\n",
    "plot_psychometric_curves(ev[:, start_per:], perf[:, start_per:],\n",
    "                         action[:, start_per:], blk_dur=block_dur,\n",
    "                         figs=True)\n",
    "# plot learning\n",
    "ev = np.reshape(data['evidence'], (1, data['evidence'].shape[0])).copy()\n",
    "perf = np.reshape(data['performance'],\n",
    "                  (1, data['performance'].shape[0])).copy()\n",
    "action = np.reshape(data['action'], (1, data['action'].shape[0])).copy()\n",
    "stim_pos = np.reshape(data['stims_position'],\n",
    "                      (1, data['stims_position'].shape[0])).copy()\n",
    "plot_learning(perf, ev, stim_pos, action, view_fig=True)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(stim_pos[:, -800:], aspect='auto')\n",
    "plt.figure()\n",
    "plt.imshow(ev[:, -800:], aspect='auto')\n",
    "plt.figure()\n",
    "plt.imshow(action[:, -800:], aspect='auto')\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12.2333px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "780.333px",
    "left": "1498.42px",
    "top": "111.483px",
    "width": "280.533px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "325.85px",
    "left": "1499.42px",
    "right": "20px",
    "top": "120px",
    "width": "281.767px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
